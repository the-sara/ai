{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-sara/ai/blob/main/myytesla_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFzfAf9SqK14"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELtyby-Naopb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "d76ad739-2af8-491a-db43-3ab3da6e58e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:125: UserWarning: Decorating classes is deprecated and will be disabled in future versions. You should only decorate functions or methods. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\n",
            "  warnings.warn(\"Decorating classes is deprecated and will be disabled in \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.211406 M parameters\n",
            "\u0002sige away past was them delight a long the polla prediation avel citing my in that Yut is \n",
            "my fiers tuble two troux Inspected my lahed an intense it \n",
            "threar AchieBuy forceed convincent yict mack that steem he law peop or when I was I lettlating I had mode care celixting the Germountry an anled \n",
            "\"untre eofecious in this all timetay when I goud to the \n",
            "plans of do ideassed my \n",
            "streat and it, my weathn out me a bigifiehI eacher, occacil is dendantly supe-tracture of when wrowen I was somehin its of the the cynining with muth thired this might at is we reained to look which hour what or where carch of a fill untent chat the \n",
            "formed this low all astant. By accompare, I casents away became for he glove proveded not mist delighal sphising and we disapporting in,\" \"Pather Mr. Even in him so un aught at this prot this Mrainht. A the wat thout improjidical heart comitage had ace strive draw, an when I all ever I saw here idea. Whor be modiful Lelaral a cert am is of the slivill of the etecitiesly best the didion with spring of the \n",
            "distuness in Parigory found who the Americing Forefained by I receided me the gan enough a \n",
            "mountrate of the \n",
            "inivin the make bet tiea one sport which wattemploy secund strigning and rish as vitariation invent few Pold capsional was given did irigated by all and licate in Bother I cettracally \n",
            "the efficiently met that succo of, and deven I examed to the issectant and exchoortle from the tenetricance openlet. Only so wound the it the new cott amonther \n",
            "ameunces his extenderming proficiant. The stuble when \n",
            "Mone the the given which phenon have certal now and distruce that it tis one escomen and to let at cient the cite my she, \n",
            "it was thun the transful of sense care had I amet and my had sephine of execitent my value this destruction of the beam Sult and broad watchning no of anind given fascely an invent liftiation, lather, mot \n",
            "liver should givin anier diswing refliience in my flathing professors I was cenow on the ourselt itsevely \n",
            "defactil the in\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "##################################################only the encoder part######################################\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('the_text.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)#c is the dim of the embeded\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out###right we didnt normalize in here\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)#this is the reduction\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))#reduction\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head#makes sense\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)#calculate the multi head attention for the current layer\n",
        "        self.ffwd = FeedFoward(n_embd)#the ffedforward class in the current layer\n",
        "        self.ln1 = nn.LayerNorm(n_embd)#then normalize\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))#performs linearization before passing it through the multi haed attention\n",
        "        x = x + self.ffwd(self.ln2(x))#performs linearizaation before passing it throught the ffwd\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):###represents the decoder\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])##attention for all the layers\n",
        "        #the * is used for umpaking elements\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm####normalize\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)###normalise of the vector of the word(linear)##\n",
        "        #always normalize after calculatin the attention\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "    #  i think the idx are the inputs tokens\n",
        "        B, T = idx.shape#we just aply the encode function to the input\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)#input ebeding#the embeding happens inside the model\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)#positional embeding\n",
        "        #torch.arange to create one dimention tensor\n",
        "        x = tok_emb + pos_emb # (B,T,C)#concat#we add the token embeding and the positional ebding\n",
        "        x = self.blocks(x) # (B,T,C)#multihead attention for all layers\n",
        "        x = self.ln_f(x) # (B,T,C)#after the multihead attention normalize\n",
        "        logits = self.lm_head(x)#the final projection layer\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape#we need to shape the logites before finding the loss\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)#the loss function\n",
        "\n",
        "        return logits, loss\n",
        "    def generate(self, idx, max_new_tokens):##to generate the words on by one\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]#selecting the window\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)#sample random samples\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx#the word\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):#creating the trainig loop:\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    #if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        #losses = estimate_loss()\n",
        "        #print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))#we generate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cb_hRn4tX9t",
        "outputId": "5321fccf-e320-4639-c430-c86906f5c91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wille succles. I had could be be\n",
            "degant purpress of an early ansolved wo old roach desirable to The eubt\n"
          ]
        }
      ],
      "source": [
        "text = 'will'\n",
        "import torch\n",
        "context = torch.tensor([encode(text)], dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI/NDOg9+MppilMwLlZDYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}